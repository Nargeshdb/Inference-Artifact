# Artifact for: Inference of Resource Management Specifications

This artifact provides a Docker image that contains:

* source code of our tools
* all benchmarks 
* scripts to reproduce our experiments
* manual analysis report of warnings generated by the verifiers

### Software Versions
* [dotnet](https://dotnet.microsoft.com/en-us/download): 6.0.119
* [CodeQL](https://codeql.github.com/): 2.13.5
* [Ubuntu](https://ubuntu.com/): 22.04 for Docker image and host machines


### Setup

  1. Install Docker based on your system configuration: [Get Docker](https://docs.docker.com/get-docker/).
  2. Import the artifact into Docker: `docker load --input oopsla-artifact-471.tar`
  3. Run the Docker image: `docker run -it --user oopsla oopsla-artifact-471 /bin/bash`

Additional packages can be installed as usual via `apt update && apt install <package>`.

Our paper's goal is to infer resource management specifications that can be utilized by our verifiers Resource Leak Checker (RLC) for Java and RLC# for C# code. Our tool is available at [RLC#](https://github.com/microsoft/global-resource-leaks-codeql).

## Evaluation of Inference and Resource Leak Checker for C# Code

We have developed both inference and RLC# as CodeQL queries.
For the evaluation of inference and RLC#, we utilize two open-source projects: [Lucene.Net](https://github.com/apache/lucenenet) and [EF Core](https://github.com/dotnet/efcore). This artifact does not include proprietary C# microservices, which are discussed in the paper.

We tested the container on a Windows-based system equipped with an Intel Xeon(R) W-2145 CPU operating at 3.7 GHz and 64 GB of RAM. We provide an estimated duration for each script to execute on this particular system. Nonetheless, the actual time taken might differ based on your machine's configuration.

A CodeQL database is a relational database that presents a project's source code in a structured format, allowing for powerful code analysis using the CodeQL query language. Creating a CodeQL database is a one-time task.

### CodeQL Database Creation

The `createCodeQLDB.py` script clones the repositories of Lucene.Net and EF Core and generates their respective CodeQL databases. It takes approximately 25-30 minutes to create both databases.
```python
python3 scripts/createCodeQLDB.py
```

### Reproducing Table 1

Table 1 in our paper presents the percentage of hand-written annotations that our algorithm was able to infer.
You can reproduce Table 1 by executing the `csharp-table1.py` script. This script runs the CodeQL query for inference on both benchmarks and compares the inferred annotations from the query with the manually added annotations (available in the docs directory). The output format is identical to that in the paper. Generating the results takes approximately 20-22 minutes. The script's output is displayed on stdout and also saved in the file `~/csharp/results/overall-results/table1.txt`.

Note that there was a bug in the script that counted the number of `MustCall` annotations on class. The updated script gives the correct numbers (which is one less than the hand-written annotations for each benchmark). Also, for EF Core, one of the Non-ReadOnly field was counted as ReadOnly. This will be reflected by the difference in numbers in the second and third column in Table 1 for EF Core. We will update these changes in the revised paper. The numbers do not change for proprietary C# microservices. 
```python
python3 scripts/csharp-table1.py
```
### Reproducing Table 2

Table 2 in our paper presents the number of warnings generated by our verifiers in two scenarios: without annotations and with inferred annotations. In our paper's Table 2, the second and third columns display these numbers for each benchmark. The remaining columns in Table 2 involve manual analysis of the warnings and cannot be generated automatically. We offer this information separately in a Google Doc [ManualAnalysis]().

To reproduce Table 2, execute the `csharp-table2.py` script. This script runs RLC# twice for each benchmark, first without annotations and then with inferred annotations calculated in the previous step. The script's output generates the first three columns of Table 2. It takes approximately 4.5 hours for the script to produce results for both benchmarks. The script's output is shown on stdout and also saved in the file `~/csharp/results/overall-results/table2.txt`.
```python
python3 scripts/csharp-table2.py
```
### Reproducing Table 3 

In our paper, Table 3 presents the performance of inference and checking. To reproduce Table 3, execute the `csharp-table3.py` script. This script runs inference and RLC# with inferred annotations three times, recording the average runtime for each query and benchmark. The output of this script generates Table 3, excluding the kLoC column. It takes approximately 8.5 hours for the script to produce results for both benchmarks. The script's output is shown on stdout and also saved in the file `~/csharp/results/overall-results/table3.txt`.
```python
python3 scripts/csharp-table3.py
```

## Evaluating Inference and Resource Leak Checker on Various C# Benchmarks

You can test both inference and RLC# on benchmarks that are not included with this artifact.

### CodeQL Database Creation

If you want to create a CodeQL database for a different project, use the following command (for more details [codeql database create](https://docs.github.com/en/code-security/codeql-cli/using-the-codeql-cli/creating-codeql-databases))
```python
cd <source-code-project>
codeql database create <database> --language=<language-identifier> 
```
You must specify:

* \<database\>: a path to the new database to be created. This directory will be created when you execute the command. You cannot specify an existing directory.
* \<language-identifier\>: the identifier for the language to create a database for. For C#, it is csharp.

### Inference

To run the inference algorithm on a different C# project, use the `inference.py` script. This script executes the inference query on a CodeQL database and generates two files in the `~/csharp-results/inference` directory. One file (named \<db-name\>-inferred-attributes.csv) contains a list of annotations inferred by the query in a format that the RLC# verifier understands. The second file (named \<db-name\>-inference-summary.csv) summarizes the inference run, providing the total number of inferred annotations and the runtime of a single query run.
```python
python3 scripts/inference.py <path-of-codeql-db>
```
### RLC\#

To run RLC# on a different C# project, use the `rlc.py` script. This script executes RLC# on a CodeQL database and generates two files in the `~/csharp-results/rlc` directory. One file (named \<db-name\>-rlc-warnings-with-\<no|manual|inferred\>-annotations.csv) contains a list of warnings generated by RLC#. Each entry corresponds to one warning, providing meta information and the location where the resource was allocated, which may not be disposed along some path. The second file (named \<db-name\>-rlc-summary-with-\<no|manual|inferred\>-annotations.csv) summarizes the RLC# run, offering the total number of generated warnings (including those related to annotation verification), actual resource leaks, and the runtime of a single RLC# run.
```python
python3 scripts/rlc.py <path-of-codeql-db> <0|1|2>
```
`0`: RLC# with no annotations

`1`: RLC# with manual annotations

`2`: RLC# with inferred annotations

### Performance 

To measure the performance of inference and RLC# on a different C# project, use the `time-inference.py` and `time-rlc.py` scripts. These scripts run inference and RLC# on a CodeQL database for a specified number of times. Both scripts generate a file reporting the runtime for each run and an average of all runs in the `~/csharp-results/inference` and `~/csharp-results/rlc` directories.
```python
python3 scripts/time-inference.py <path-of-codeql-db> <num-of-runs>
python3 scripts/time-rlc.py <path-of-codeql-db> <0|1|2> <num-of-runs>
```
